[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tampa Bay Open Science Workshop",
    "section": "",
    "text": "Course synopsis\nWelcome to the Tampa Bay open science workshop. Open science (OS) has been advocated as an effective approach to create reproducible, transparent, and actionable research products. However, widespread adoption among the research and management community has not occurred despite its perceived benefits. In the face of major challenges like global warming and sea level rise, the collaborative framework provided by OS is needed now more than ever. This workshop will cover material introducing participants to core concepts of OS. The target audience includes anyone interested in applying OS in their own workflows as part of a larger research and resource management team.\nBy the end of this workshop, you should have a solid understanding of fundamental concepts in open science and how they can be applied to help bridge the research-management divide. You will also have the skills to understand how collaborative open science tools can be used to increase efficiency and transparency, understand fundamental best practices for working with data to facilitate openness, and be able to apply these lessons within your own teams by effectively addressing barriers to adoption.\nMuch of the content on this web page was adopted from the TBEP Data Management SOP."
  },
  {
    "objectID": "index.html#prepare",
    "href": "index.html#prepare",
    "title": "Tampa Bay Open Science Workshop",
    "section": "Prepare",
    "text": "Prepare\nPlease attend the workshop with a personal laptop and power supply. Make sure your laptop can access publicly avialable WiFi. In addition, create an account for yourself on GitHub and ORCID:\n\nGitHub create account: link\nORCID create account: link"
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Tampa Bay Open Science Workshop",
    "section": "Agenda",
    "text": "Agenda\n\nThe basics of open science: 9am - 10am\nOpen science for collaboration: 10am - 12pm\nOpen science for impactful products: 12:30pm - 2:30pm\nLowering barriers to inclusion and addressing key critiques: 2:30pm - 3pm\n\nEach module uses a set of common icons to orient you to specific tasks or experiences during this workshop. These include the following:\n\n Exercise and discussion\n\n\n Watch and learn\n\n\n Description of a collaborative tool\n\n\n Pros of a collaborative tool or solution to an open science challenge\n\n\n Cons of a collaborative tool\n\n\n Challenge to overcome for open science"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Tampa Bay Open Science Workshop",
    "section": "Instructors",
    "text": "Instructors\nDr. Chris Anastasiou is a Chief Water Quality Scientist and the Seagrass Mapping Program Lead for the Southwest Florida Water Management District. He holds a PhD in Marine Science from the University of South Florida and has been working in and around the springs of Florida for more than 25 years.\nDr. Marcus Beck is the Program Scientist for the Tampa Bay Estuary Program and is developing data analysis and visualization methods for Bay health indicators. He received his PhD in Conservation Biology from the University of Minnesota in 2013. Marcus has experience researching environmental indicators and developing open science products to support environmental decision-making. Marcus is also an open source software and dashboard developer to facilitate science application.\n\n  This website is licensed under a Creative Commons Attribution 4.0 International License.\n This version of the website was built automatically with GitHub Actions on 2023-02-01."
  },
  {
    "objectID": "basics.html#goals-and-motivation",
    "href": "basics.html#goals-and-motivation",
    "title": "1  Basics of open science",
    "section": "1.1 Goals and motivation",
    "text": "1.1 Goals and motivation\nThis is the first module in our workshop on open science. This module describes the need for open science, how it can improve research applications, and exposes you to common ideas and terminology that we’ll be using throughout the day. Consider this your 30,000 foot view of open science. Our later modules will provide more detail on specific topics in open science that you can use for continued learning.\n\nGoal: get comfortable with key ideas and concepts for understanding open science\nMotivation: This is the first step in your open science journey!"
  },
  {
    "objectID": "basics.html#why-open-science",
    "href": "basics.html#why-open-science",
    "title": "1  Basics of open science",
    "section": "1.2 Why open science?",
    "text": "1.2 Why open science?\nLet’s start with revisiting the scientific process. I’m sure this looks familiar to all of you. This is geared towards an applied research question.\n\nOur basic scientific approach to discovery is motivated by a question or research goal, developing a hypothesis for the question, collecting data based on the hypothesis, developing a tool that can be used for decision-making, and summarizing the results in a conventional format.\nIn a little more detail, your workflow may look something like this.\n\nMany scientists, especially early career researchers (my past self included), may assume that this is sufficient to affect change. We write the report, send it out into the world, and move on to the next project. This is a common mentality:\n\n“This 500-page report will answer all of their questions!”\n\nFrom the other side, such as the manager or policy-maker, the report may be received like this:\n\n“This 500-page report answers none of my questions!”\n\nIt’s dense, inaccessible, and there are probably questions about the underlying data and methods used to achieve the results. More importantly, it doesn’t present the information in an easily digestible format to quickly make the right decision. Sometimes, if you think you’re doing applied science, it may just be implied science that falls short of application.\nWhy is this conventional approach to science ineffective at seeding change?\nThe environmental management community is often siloed with each branch doing their own thing and speaking their own language. Between the research (typically academic) and management community, we call this the research-management divide.\n\nA distinct gap exists between how scientific products are developed and how they can be used to meet management needs. This is often the result of communication barriers, irreproducible results, information loss with poor documentation, inaccessible data, and opaque workflows known only to the analyst.\nThese barriers can occur at any stage of the research process. This compelling graphic from Michener et al. (1997) describes the atrophy of information in a closed approach to creating science.\n\nThe last part is especially morbid. Sometimes, this is called the “bus factor”. What would happen to your important work and life achievements if you were hit by a bus? Would others be able to pick it up? Research products with a high bus factor are at risk of being lost if critical team members are no longer available. This is a very real problem for continuity of science.\n\nSo how do we make changes to our workflows to ensure we can achieve truly applied science using open tools and philosophies?"
  },
  {
    "objectID": "basics.html#learning-and-speaking-the-language-of-open-science",
    "href": "basics.html#learning-and-speaking-the-language-of-open-science",
    "title": "1  Basics of open science",
    "section": "1.3 Learning and speaking the language of open science",
    "text": "1.3 Learning and speaking the language of open science\nThe tools and broader philosophy behind open science can help us bridge the research-management divide. It involves a fundamental shift in how we approach the scientific process, both for your own internal workflows and how you can engage others in the process. By others, we mean not just researchers, but specifically those that need the information to make informed decisions. This also includes your future self.\nBefore we present a formal definition, let’s describe a modification of the conventional workflow that includes an open process to discovery and implementation (Beck et al. 2020; modified from Hampton et al. 2015).\n\nThis workflow is similar to the original scientific method, but the technical components are open to managers and stakeholders, we’re treating data differently by using metadata and archiving, we’re creating summary documents that include source code with text, and we’re producing decision-support tools to meet the needs outside of the research community. Importantly, the process is also iterative and dynamic.\nThroughout this workshop, we’ll learn about some open science tools that can be used in this generalized workflow to achieve better science in less time (Lowndes et al. 2017).\nNow let’s settle on a definition for open science (from Open Knowledge International, http://opendefinition.org/, https://creativecommons.org/):\n\n“The practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely available, under terms that enable reuse, redistribution and reproduction of the research and its underlying data and methods.”\n\nKey words from this definition are italicized. There are very specific tools in the open science toolbox that enable each of these key words. We’ll cover some of these later.\nSimilarly, the current administration has declared 2023 the Year of Open Science. Their definition is:\n\n“The principle and practice of making research products and processes available to all, while respecting diverse cultures, maintaining security and privacy, and fostering collaborations, reproducibility, and equity.”\n\nWe can can breakdown these definitions into key principles.\n\n Open data\n\nPublic availability of data\nReusability and transparent workflows\nData provenance and metadata\n\n Open process\n\nIterative methods using reproducible workflows\nCollaboration with colleagues using web-based tools\nLeveraging external, open-source applications\n\n Open products\n\nInteractive web products for communication\nDynamic documents with source code\nIntegration with external networks for discoverability\n\n\nYou’ll notice that web-based tools and open science are often discussed at the same time. Science existed before the internet. Open science often focuses on how the two can leverage and support one another despite the latter being a relatively new addition to society. We often describe web-based tools as synonymous with open science."
  },
  {
    "objectID": "basics.html#fair",
    "href": "basics.html#fair",
    "title": "1  Basics of open science",
    "section": "1.4 The FAIR principles",
    "text": "1.4 The FAIR principles\nAdvocates of open science also use the FAIR principles (Wilkinson et al. 2016) as a vehicle for achieving the former. It’s important to understand what they mean so that you can be fluent in both. The FAIR acronym is described as follows:\n\nFindable: The data have a globally unique and persistent identifier, including use of “rich” metadata.\nAccessible: Once found, the data can be retrieved using standardized communications protocols that are open, free, and universally implementable.\nInteroperable: The ability of data or tools from non-cooperating resources to integrate or work together with minimal effort.\nReusable: If the above are achieved, the data and metadata are described in a way that they can be replicated and/or combined in different settings.\n\nSimply, what this means is: 1) each dataset has a name that doesn’t change and can be found with minimal effort using that name, 2) once it’s found, you can actually get your hands on it (e.g., not behind a paywall), 3) once you have it, you can use readily available tools to work with the data (e.g., not using proprietary software), and 4) you can actually apply the data for your own needs because it has sufficient context, including its reproduction, given that the first three principles are met.\nFor our purposes, think of these ideas as general guidelines you can ask yourself when doing science. If you find that your work is not FAIR, then you’re probably not being as open as you could be. We’ll of course provide some tools to help you be FAIR and open."
  },
  {
    "objectID": "basics.html#schools",
    "href": "basics.html#schools",
    "title": "1  Basics of open science",
    "section": "1.5 Schools of thought",
    "text": "1.5 Schools of thought\nFinally, it’s useful to make a distinction of how different people may talk about open science. This can help you better navigate conversations and become an advocate for open science in your own right.\nA useful paradigm is provided by Fecher and Friesike (2014) to describe open science as five distinct schools of thought:\n\nThese are of course only conceptual boxes and there’s considerable overlap across all schools when open science is used in practice. For our purposes, we’ll mostly be talking about ideas and tools from the pragmatic, infrastructure, and democratic schools of thought. The end goal is to provide you with the means to create more efficient and impactful science that can more readily be used by others in a collaborative setting.\n\n Exercise and discussion\n\nTake a few minutes to jot down your individual answers to the following questions. When you’re done, share amongst your peers at your table.\n\nHow do you currently define open science, if at all?\nWhat tools do you use in your job that facilitate collaboration or openness?\nHas any of the above changed your understanding of what open science means for research and/or resource management?\n\n\n\n\n\nBeck, M. W., C. O’Hara nad J. S. S. Lowndes, R. D. Mazor, S. Theroux, D. J. Gillett, B. Lane, and G. Gearheart. 2020. “The Importance of Open Science for Biological Assessment of Aquatic Environments.” PeerJ 8: e9539. https://doi.org/10.7717/peerj.9539.\n\n\nFecher, B., and S. Friesike. 2014. “Open Science: One Term, Five Schools of Thought.” In Opening Science, 17–47. Springer, Cham.\n\n\nHampton, S. E., S. S. Anderson, S. C. Bagby, C. Gries, X. Han, E. M. Hart, M. B. Jones, et al. 2015. “The Tao of Open Science for Ecology.” Ecosphere 6 (7): 1–13. https://doi.org/10.1890/ES14-00402.1.\n\n\nLowndes, J. S. S., B. D. Best, C. Scarborough, J. C. Afflerbach, M. R. Frazier, C. C. O’Hara, N. Jiang, and B. S. Halpern. 2017. “Our Path to Better Science in Less Time Using Open Data Science Tools.” Nature Ecology & Evolution 1 (0160): 1–7. https://doi.org/10.1038/s41559-017-0160.\n\n\nMichener, W. K., J. W. Brunt, J. J. Helly, T. B. Kirchner, and S. G. Stafford. 1997. “Nongeospatial Metadata for the Ecological Sciences.” Ecological Applications 7 (1): 330–42. https://doi.org/https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2.\n\n\nWilkinson, M. D., M. Dumontier, I. J. Aalbersberg, G. Appleton, M. Axton, A. Baak, N. Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (160018). https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "collaborate.html#goals-and-motivation",
    "href": "collaborate.html#goals-and-motivation",
    "title": "2  Open science for collaboration",
    "section": "2.1 Goals and motivation",
    "text": "2.1 Goals and motivation\nThis is the second module in our workshop on open science. This module will explore some open science tools to help you and your team become better collaborators and to better engage your science with external partners. We’ll introduce some essential elements of collaboration and discuss some readily available tools for doing so.\n\nGoal: understand methods of collaboration and the pros/cons of various tools\nMotivation: start building the tools for your open science toolbox"
  },
  {
    "objectID": "collaborate.html#essential-elements-of-collaboration",
    "href": "collaborate.html#essential-elements-of-collaboration",
    "title": "2  Open science for collaboration",
    "section": "2.2 Essential elements of collaboration",
    "text": "2.2 Essential elements of collaboration\nWe start our deep dive into open science by focusing on collaboration as a fundamental activity that can be enhanced through transparent, efficient, and reproducible tools. Having effective tools to work together is a critical theme of many open science practices. There are many tools in the toolbox and we need to introduce some core concepts before we demonstrate how to implement them in practice.\n\n2.2.1 Workflow management\nHow do you organize your work each day? How do you make sure projects are on schedule and pressing deadlines are met? How do you plan for short-term and long-term goals? Do you have a five-year, ten-year, or longer career plan?\nWork to achieve goals cannot be accomplished without a systematic approach to organizing tasks. Chances are, we each have our own system that works for us and was probably developed through trial and error. Although everyone has familiar workflows, they are often idiosyncratic and deeply entrenched by habit. That can be in direct conflict with collaboration when we try to mesh internal workflows with those of others.\nDoes this look familiar?\n\n\n\n\n\n\n\n\n\nAlthough the above comic from xkcd speaks directly to file management, it hints at a broader problem of personal information management that can seriously complicate working with others. I’m sure we’ve all struggled to find that one file for that one project from a vague recollection of seeing it a few months ago.\nCollaborative work can be facilitated through workflow management that helps you break out of old habits. We’ll introduce some specific internet-based tools below to facilitate workflows either for yourself or, better yet, working with others. These can help propel you towards open science.\nHere, we introduce the Kanban approach to workflow management. The idea is simple. Create a task-oriented workflow using a card management system organized by progress. It looks something like this:\n\n\n\n\n\n\n\n\n\nAs shown, this approach can work as a literal, physical board or as one used digitally through a web browser or other software. Every Kanban board has the following elements that allow you to work in a more informed manner:\n\nProvides a “big picture” of progress\nOrganizes progress by discrete steps\nEstablishes cards as specific tasks\n\nMany of the open science tools we describe below use this system. It is a generalizable format that works in different settings, whether it be general project management or something more formal like software development.\n\n\n2.2.2 Version control\nA specific problem for workflow management that can be solved by open science tools is file management. Workflows can be immensely enhanced by tools that use strict guidelines for tracking changes and allowing a complete view of the evolution of a project. This is where version control comes in.\nI’m sure many of you have fallen into this trap:\n\n\n\n\n\n\n\n\n\nVersion control is a way to track the development history of a project. It serves the joint purposes of:\n\nFormally documenting the changes that have been made to code or software\nMaking sure that the development history is permanent\nProviding a system for collaborating across platforms (with friends!)\n\nIt’s more than saving files. Documenting changes with a set of commands that follow strict rules provides a transparent record for yourself and others, and establishing permanency ensures that any of the changes that are made can be vetted and accessed as needed. Think of it as an insurance plan for your project.\nIf you’ve ever used Google Docs, you might have noticed a feature that looks a lot like version control. The Google Drive platform is a great way to start working together and a great way to familiarize yourself with the basics of version control.\n\n\n\n\n\n\n\n\n\nFor any Google Doc, clicking on the link shown by the arrow will open the Version history pane which shows all of the edits that were made to the document. You can view any of the edits, who made the edits, view the changes (before/after) in the document, or even restore the document to a previous version.\n\n\n\n\n\n\n\n\n\nThese are the building blocks of version control as demonstrated with Google Docs:\n\nNo iterative and ambiguous file naming\nHistory of changes assigned to each editor\nAbility to restore a previous version\n\nPerhaps more importantly, these tools are in the cloud and openly accessible (unlike other cloud-based services). File links (via a URL) also do not change if a file is moved to a different location in the drive. Overall, the Google platform is an accessible means of improving collaboration (but not without it’s cons).\n\n\n2.2.3 Git and GitHub\nAlthough Google products can get you a long way towards better collaboration, they do not use dedicated version control software. These tools become more important as your projects become more complex - those beyond simple documents or spreadsheets.\nThe most widely used software for version control is Git. Although we do not cover the specifics of this software, it’s useful to understand the purpose and what it can do in making your work more open and impactful. Git is integrated with many popular open source development platforms, such as RStudio.\nMany people often confuse Git with GitHub. GitHub is an online platform for working collaboratively through Git AND it allows you to be open with your work. We’ll provide some examples below of how this can be done. Importantly, you do not need to be an expert in Git to be able to use GitHub. This speaks volumes for how team efficiency can be improved with GitHub through better collaboration.\nThis recent blog provides a helpful introduction to Git/GitHub for the casual user.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctocat, the strange and loveable mascot of GitHub.\n\n\n\n\n\n Challenge\n\nMany institutions block access to Google products or GitHub. See some ideas in our last module to overcome this issue.\n\n Watch and learn\n\nWorkflow management in the real world - using GitHub to collaborate. Here we present some examples from the Tampa Bay Estuary Program State of the Bay report and water quality report card.\n\n Watch and learn\n\nNow we’ll demonstrate how to setup a version control project with RStudio, Git, and GitHub. This example will cover:\n\nCreating the project in GitHub\nCreating a file, adding content, and committing it to the project\nSetting up issues in GitHub\nAdding members to the project\nCreating a Kanban project board to assign tasks\n\n\n\n2.2.4 Code of Conduct\nEvery responsible collaborative team begins work by creating a Code of Conduct. This documents a set of community and social standards within which the work can be completed. It ensures all viewpoints are heard and respected and establishes a means by which conflicts can be resolved.\nHere’s a great example from our friends at openscapes and one from the ROpenSci community. Some guidance for creating a code of conduct is also provided in this blog post from ROpenSci.\nThe goal of every code of conduct is to ensure an agreed upon set of norms are used by all team members to help create a safe and positive experience."
  },
  {
    "objectID": "collaborate.html#additional-tools-for-collaboration",
    "href": "collaborate.html#additional-tools-for-collaboration",
    "title": "2  Open science for collaboration",
    "section": "2.3 Additional tools for collaboration",
    "text": "2.3 Additional tools for collaboration\nBelow we introduce additional web-based tools that you can use to improve collaboration and openness, including those described above. We present them as a suite of options to consider based on the pros and cons associated with each tool. This is by no means a comprehensive list, but it should get you started towards better collaboration in an open environment.\n\n2.3.1 Slack\n\n\n\n\n\nhttps://slack.com/\n\n What\n\nAn online messaging platform for internal communication. Conversations can be organized by topic (via channels) or you can send direct messages to one or more team members. You can have multiple workspaces for different groups.\n\n Pros\n\nAlleviate email overload through quick, informal messaging. Offers a fresh approach to online communication.\n\n Cons\n\nYet another thing to monitor. Free subscription limits archive of messages. Communication is limited to those in the same workspace.\n\n\n2.3.2 Trello\n\n\n\n\n\nhttps://trello.com/\n\n What\n\nA Kanban style workflow organization platform. Can be used for personal organization or in teams. Card management allows you to assign due dates, add attachments, make checklists, assign tasks to yourself or team members, and label by themes.\n\n Pros\n\nEasy to use and can upgrade with “power-ups” for integration with other services (e.g., Google). Use across locations (e.g., from home or in the office) is easy because it’s based in a web browser.\n\n Cons\n\nNot entirely open because it’s only visible to yourself or those you explicitly invite. Free version is limited to only a handful of “power-ups”.\n\n\n2.3.3 Google Drive\n\n\n\n\n\nhttps://google.com/drive\n\n What\n\nCloud-based platform for sharing documents, worksheets, slides, etc. Follows a familiar file-based structure that is common to most operating systems.\n\n Pros\n\nEasy to use and can be a very open space for collaboration. Fairly interoperable with different file formats. Some functionality with version control (i.e., ability to “revert” to previous versions and to view changes).\n\n Cons\n\nRequires a Google account and access can be tricky depending on institution. Even though some versioning is provided, the format can encourage poor file management. Who knows what Google is doing with your data.\n\n\n2.3.4 Office 365\n\n\n\n\n\nhttps://www.microsoft.com/en-us/microsoft-365\n\n What\n\nCloud-based platform for secure sharing of Microsoft documents, worksheets, slides, etc.\n\n Pros\n\nEasy to use and fully supports Microsoft products. Low barrier of inclusion to others that are already using Microsoft products.\n\n Cons\n\nRequires a Microsoft account and access can be tricky depending on institution. Maintains dependency on expensive Microsoft products that aren’t reproducible or interoperable. Very often used in closed workflows.\n\n\n2.3.5 GitHub\n\n\n\n\n\nhttps://github.com\n\n What\n\nCloud-based platform for sharing code with Git version control. Supports sharing of most file types, although code and text-based files are the primary use.\n\n Pros\n\nCollaborative and fully transparent work environment for files under version control. Supports workflow management through issue tracking and Kanban style project boards. Links to third-party platforms for archiving and DOI generation (e.g., Zenodo). Octocat mascot is super cute.\n\n Cons\n\nLearning curve is steep if you want to fully leverage version control. Not a formal data archival service by itself and file sizes are limited.\n\n Exercise and discussion\n\nIn small groups, setup a shared workspace using GitHub and create a project management board. Some real world examples of why you might do this were presented in the earlier watch and learn.\n\nOpen GitHub in a web browser and have one person create a new repository (the big, green “New” button in Repositories). Add each member to the repository after it’s created (hint: Settings -> Collaborators)\nHave that same person create a project board for the repository (Hint: Projects -> New project -> board format)\nAfter each person accepts the invitation to the repository (check your email!), each new member create a new file in the repository (Hint: Click “Add file” near the top). Name it something unique, save and commit the changes\nAssign issues to different members of the repository to do something to the new files (Hint: on the right menu, select “Assignees”). Add the issue to the project board (Hint: on the right menu, select “Projects” and click the new project).\nWork on the issues until the time is up. Close each issue as they’re completed."
  },
  {
    "objectID": "impact.html#goals-and-motivation",
    "href": "impact.html#goals-and-motivation",
    "title": "3  Open science for impactful products",
    "section": "3.1 Goals and motivation",
    "text": "3.1 Goals and motivation\nThis is the third module in our workshop on open science. Now we focus on core principles for data management as the foundation for open science. We discuss the role of data management to support decisions using open science. Then, we introduce the concepts of tidy data as a unified format for storing information. We close with a discussion of metadata tools and data repositories that allow your data to live beyond the project.\n\nGoal: understand best practices for data management as a key concept for open science\nMotivation: cultivate data as a living, shared resource"
  },
  {
    "objectID": "impact.html#data-as-the-foundation-for-open-science",
    "href": "impact.html#data-as-the-foundation-for-open-science",
    "title": "3  Open science for impactful products",
    "section": "3.2 Data as the foundation for open science",
    "text": "3.2 Data as the foundation for open science\nIn the last module, we talked about collaboration as the single most important activity of open science. So, why are we now talking about data management? Understanding the tools of collaboration allows you to better engage with your colleagues and partners, but open engagement will mean nothing if your data look like garbage.\n\n\n\n\n\n\n\n\n\nYou can probably recall past instances when poor data management has been a challenge for open collaboration. Here are a few real-world examples:\n\nA collaborator calls you on the phone asking about a historical dataset from an old report. You spend several hours tracking down this information because you don’t know where it is. The data you eventually find and provide to your collaborator has no documentation and they don’t know how to use it or use it inappropriately.\nYou receive a deliverable from a project partner that was stipulated in a scope of work. This deliverable comes in multiple formats with no reproducible workflow to recreate the datasets. You are unable to verify the information, eroding your faith in the final product and making it impossible to update the results in the future.\nAn annual reporting product requires using new data each year. The staff member in charge of this report spends several days gathering the new data and combining it with the historical data. Other projects are on hold until this report is updated. Stakeholders that use this report to make decisions do not trust or misunderstand the product because the steps for its creation are opaque.\n\nData come in many shapes and sizes, most more like the right side of the above picture. Poor data management occurs for many reasons, but these are a few of the common causes:\n\nWhat’s easy for recording data doesn’t usually translate to easy analysis\nEgregious use of Excel as data management software\nMetadata is a chore that is often an afterthought\n\nIt’s often said that 90% of working with data is cleaning (or “wrangling”), whereas actual analysis and interpretation is a small fraction of your total effort. Using better data management practices will not only help you save time, it’s also a service for your colleagues, potential collaborators, and your future self. Therefore, better data management leads to more open science.\nThe FAIR principles outlined in the first module are especially useful when working with data for open science applications. Many of the collaborative tools in the second module can facilitate application of FAIR data. In this module, we’ll go a step further to discuss how data structure, including metadata, can produce a FAIR dataset."
  },
  {
    "objectID": "impact.html#tidy",
    "href": "impact.html#tidy",
    "title": "3  Open science for impactful products",
    "section": "3.3 Principles of tidy data",
    "text": "3.3 Principles of tidy data\nTabular data allow you to store information, where observations are in rows and variables are in columns. It’s very common to try to make tabular data more than it should be. Unless you spend a lot of time working with data, it can be difficult to recognize common mistakes that lead to table abuse.\nBefore we get into tidy data, we need to discuss some of the downfalls of Excel as a data management system. There are many examples that demonstrate how Excel has contributed to costly mistakes through table abuse or outright negligence, often to the detriment of science (Ziemann, Eren, and El-Osta 2016).\n\n\n\n\n\n\n\n\n\nExcel allows you to abuse your data in many ways, such as adding color to cells, embedding formulas, and automatically formatting cell types. This creates problems when the organization is ambiguous and only has meaning inside the head of the person who created the spreadsheet. Embedding formulas that reference specific locations in or across spreadsheets is also a nightmare scenario for reproducibility.\n\n\n\n\n\n\n\n\n\nIf you absolutely must use Excel to store data, the only acceptable format is a rectangular, flat file. This is typically saved as a .csv file. What do we mean by this?\nA rectangular file:\n\nStore data only in rows and columns in matrix format (e.g., 10 rows x 5 columns), with no “dangling” cells that have values outside of the grid or more than one table in a spreadsheet.\n\nA flat file:\n\nNo cell formatting, no embedded formulas, no multiple spreadsheets in the same file, and data entered only as alphanumeric characters.\n\nBroman and Woo (2018) provide an excellent guide that expands on these ideas. Essentially, these best practices force you to isolate the analysis from the data - many people use Excel to mix the two, leading to problems.\nNow we can talk about tidy data. The tidy data principles developed by Hadley Wickham (Wickham 2014) are a set of simple rules for storing tabular data that have motivated the development of the wildly popular tidyverse suite of R packages (Wickham et al. 2019). The rules are simple:\n\nEach variable must have its own column;\nEach observation must have its own row; and,\nEach value must have its own cell.\n\nGraphically, these rules are shown below (from Wickham and Grolemund 2017):\n\n\n\n\n\nUsing these principles may seem unnatural at first because of a difference between what’s easy for entering data versus what makes sense for downstream analyses. For example, dates are often spread across multiple columns, such as having one column for each year of data where the header indicates the year that applies to data in the column.\nThe following examples show five tables represented in different arrangements. Only one of the tables is tidy - which one?\n\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n\n\n\n\n\n\n\nTable 3\n\n\n\n\n\n\n\n\n\n\nTable 4a\n\n\n\n\n\n\n\nTable 4b\n\n\n\n\n\nOnly the first table is tidy - each variable has its own column, each observation has its own row, and each value has its own cell. Table 2 violates the first rule, Table 3 violates the third rule, and tables 4a and 4b violate the first and second rules.\nTable 2 is a special case where the data are mostly tidy, but represented as a long format of the same wide data in Table 1. Storing data in a long format is not necessarily wrong and, in fact, is a common format for databases. There are pros and cons to both, depending on the intended analysis. Here are two examples of some water quality data you might encounter in the wild, one wide and one long.\nUsing a tidy format also allows you to more easily join data between tables. This is a common task when you have information spread between different tables because: 1) it might not make sense to keep the data in the same table, and 2) the analysis depends on information from both tables. Tidy data shared between tables can be linked using a “key” as a common identifier.\n\n\n\n\n\n\n Watch and learn\n\nMaking an untidy dataset tidy using Excel (an irreproducible example).\n\n Watch and learn\n\nMaking an untidy dataset tidy using R (a reproducible example).\n\n\nShow the code\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# import and wrangle\ndat <- read_excel('data/untidy.xlsx', skip = 1) %>% \n  fill(Location) %>% \n  pivot_longer(cols = `2019`:`2021`, names_to = 'Year', values_to = 'Acres/Category', names_transform = as.integer) %>% \n  separate(col = `Acres/Category`, into = c('Acres', 'Category'), sep = '/', convert = T)\ndat\n\n\n# A tibble: 27 x 5\n   Location  Habitat   Year Acres Category\n   <chr>     <chr>    <int> <int> <chr>   \n 1 Clear Bay Seagrass  2019   519 B       \n 2 Clear Bay Seagrass  2020   438 C       \n 3 Clear Bay Seagrass  2021   375 A       \n 4 Clear Bay Oysters   2019   390 B       \n 5 Clear Bay Oysters   2020   875 B       \n 6 Clear Bay Oysters   2021   724 A       \n 7 Clear Bay Sand      2019   742 C       \n 8 Clear Bay Sand      2020   702 A       \n 9 Clear Bay Sand      2021   505 C       \n10 Fish Bay  Seagrass  2019   930 B       \n# ... with 17 more rows\n\n\n\n Exercise and discussion\n\nNow try it on your own. Download the untidy dataset and make it tidy using your preferred software. Compare your tidied dataset with this one to see if you’ve done it right."
  },
  {
    "objectID": "impact.html#data-dictionaries",
    "href": "impact.html#data-dictionaries",
    "title": "3  Open science for impactful products",
    "section": "3.4 Data dictionaries",
    "text": "3.4 Data dictionaries\nOnce you understand the tidy principles, you’ll find that analysis is much, much easier. More importantly, this also facilitates documentation for the explicit purpose of open sharing. As responsible data stewards, we need to think of data as a living resource for open collaboration that is not just a means for more conventional scientific products (e.g., a publication). Data are increasingly being documented and cited as unique entities and we need proper documentation to help support the FAIR principles.\nA good first step in documentation is to create a data dictionary. This will help us when we start creating metadata. Think of this as the specific description of the contents of a tabular data file. Developing a data dictionary not only helps with metadata, but also helps you think more clearly about your data.\nA data dictionary describes column names and the type of data in each column. Simple things like how you name a data column can have larger implications for downstream analysis pipelines or interpretability of a dataset.\nHere’s an example of a data dictionary for a made up dataset. Without seeing the actual data, you get a good sense of what’s included and acceptable values for adding new data.\n\n\n\n\n\n\n\n\n\nHere we provide some general guidelines for developing your own data dictionary.\n\nColumn names\n\nBe as descriptive as possible while trying to keep the name as short as possible. Really long names with lots of detail can be just as frustrating as very short names with very little detail. The column name should be intuitive to point the analyst in the right direction.\n\nTry to avoid spaces or commas in column names since some software may interpret that as the start of a new column. Do not start a column name with a number.\n\nIt may also be useful to identify a “plot name” column that uses proper spelling, punctuation, and units. Entries in this column can be used for making graphics that have interpretable names (e.g., using “Temp. (F)” instead of “temp” as the name of a plot axis).\n\nData types\n\nDescribe the type of data in each column, e.g., numerical measurements, categorical descriptors, or counts of observations (integer). Never, ever mix data types in the same column.\n\nIf your data are continuous numeric values, try to identify an acceptable range for the values, e.g., are there minimum or maximum values that would indicate the data are out of range?\n\nFor categorical descriptors, identify all possible categories (if feasible) that are acceptable values for the column, e.g., small, medium, or large for a qualitative descriptor of size.\n\nFor dates, make note of the format, e.g., YYYY-MM-DD. For time, identify the timezone and if it includes daylight savings or not.\n\n\n\n Exercise and discussion\n\nCreate a data dictionary for the tidy dataset from the previous example. Use a spreadsheet to create a table to describe the column names, data type, acceptable values, and a description."
  },
  {
    "objectID": "impact.html#importance-of-metadata",
    "href": "impact.html#importance-of-metadata",
    "title": "3  Open science for impactful products",
    "section": "3.5 Importance of metadata",
    "text": "3.5 Importance of metadata\nHow many times have you been sent a dataset without any idea what it contains or why it was created? How are you sure the information is valid and that your analysis takes into account the limitations of the data? How many times have you willfully sent someone a dataset without fully providing this information?\nWithout metadata, it’s impossible to know critical details about a dataset that can inform its analysis, and more importantly, its use to inform decision-making. Curating data should be synonymous with metadata generation and is an important part of open science. We cannot provide open data in good faith without also providing metadata.\nMetadata is literally defined as “data about data”. It varies from simple text descriptions of a dataset, such as “who”, “what”, “when”, “where”, “why”, and “how”, to more formalized standards with that prepare your data for archiving in a long-term repository. The data dictionary is only part of a complete metadata description.\nA useful definition is provided by Gilliland (2016):\n\nA suite of industry or disciplinary standards as well as additional internal and external documentation and other data necessary for the identification, representation, interoperability, technical management, performance, and use of data contained in an information system.\n\nWhy don’t we see more metadata in the wild? Short answer is that it’s often an afterthought, if considered at all. Creating metadata is usually tedious and the return on investment is not apparent at onset of a project. However, the collective growth of sciences and its application to real world problems is dependent on metadata.\nThe US Geological Survey provides a useful document on creating Metadata in “plain language” to distill the basic information contained in a metadata file. It provides a workflow for answering the “who”, “what”, “when”, “where”, “why”, and “how” questions for metadata. Below is a brief synopsis:\n\nWhat does the dataset describe?\n\nInformation here would include very basic details about the dataset including a title, geographic extent, and period of time covered by the data. For geographic extent, this may often include explicit coordinates covering the study area. Location is useful for indexing your dataset relative to others, if for example, a researcher wanted to find data for all studies in the geographic extent of Tampa Bay.\n\nWho produced the dataset?\n\nThis would be yourself and anyone else who has made a significant contribution to the development of a dataset. Data are increasingly being used as citable resources and including individuals that were important in its generation ensures proper attribution. If someone has spent hours toiling in the field to collect the data or hours visually scanning a spreadsheet for quality control, include them!\n\nWhy was the dataset created?\n\nDescribing why a dataset was created is critically important for understanding context. If others want to use your data, they need to know if it’s appropriate for their needs. Here you would describe the goal or objectives of the research for which the data were collected. It should be clear if there are limitations in your data defined by your goals.\n\nHow was the dataset created?\n\nHere you would describe the methods used to generate the data, e.g., field sampling techniques, laboratory methods, etc. This information is important so others can know if you’ve used proper and accepted methods for generating the data. Citing existing SOPs or methods that are recognized standards in your field would be appropriate.\n\nHow reliable are the data?\n\nIt’s also important to explicitly note instances when the data could be questionable or inappropriate to use. Here you could describe any quality assurance or quality control (QAQC) checks that were used on the data. There are often formalized ways to do so, such as codes or descriptors in tabular data defining QAQC values (e.g., data in range, below detection limits, sensor out of service, etc.).\n\nHow can someone get a copy of the dataset?\n\nGood metadata has information on who to contact for getting the data. This contact may not be the same as who created the dataset (e.g., IT staff). For archived or publicly available data, this information is more important for who to contact should someone have questions. Information on obtaining a copy of the data should also describe any special software or licensing issues related to accessing the data.\nOnce you’ve gathered this information, how do you turn it into literal metadata? It depends on how deep you want to go. At it’s simplest, your metadata could be a simple text file with answers to the questions. Or it could be a specific file format used by modern data archive repositories (e.g., EML format).\nHere’s an example of a bare bones metadata file. One could easily type this up in a text file or spreadsheet.\n\n\n\n\n\n\n Exercise and discussion\n\nCreate a metadata file for the tidy dataset from the previous example. Just start by using your data dictionary and develop narrative answers for “who”, “what”, “when”, “where”, “why”, and “how” to describe your dataset. Get creative with your descriptions since this is a made up dataset. Enter this information into a new spreadsheet."
  },
  {
    "objectID": "impact.html#data-repositories",
    "href": "impact.html#data-repositories",
    "title": "3  Open science for impactful products",
    "section": "3.6 Data repositories",
    "text": "3.6 Data repositories\nHow data are treated as living, dynamic pieces of information is critical to the ethos of open science. This is especially true when the FAIR principles are invoked. Data should not live on your hard drive as something only known to yourself.\nAlthough we will not cover data repositories in depth, it’s important to recognize the critical role that data archiving and metadata have in open science. How many times have you thought “wow, it would be great if I could have the data from this paper!” Making data open is a great way to propel science through better collaboration.\nThe ease of getting a dataset online depends on where you want to put the data. In most cases, your dataset should be tidy and accompanied by metadata. For simple solutions, such as FTP hosting or putting a dataset on Google Drive, all you need to do is upload the data and metadata by hand. However, this doesn’t necessarily make it findable and the permanency is uncertain.\nThe absolute best standard for hosting data online is through a Federated Data Repository:\n\nAn online network of connected repositories that use similar standards to collectively store data for discovery and access. Uploading a dataset to one node of a repository will make it available through all other nodes.\n\nSuch repositories follow strict but necessary guidelines to ensure your data have permanence and adhere to the FAIR principles. The data are definitely findable (e.g., through a web search), accessible (free to download), and interoperable (accepted standards are ensured). The “reproducible” aspect can be debatable, but that can be solved through other means (e.g., code sharing on GitHub).\nSome examples of data repositories, most are domain-specific:\n\nKNB: Knowledge Network for Biocomplexity, a general purpose repository for ecological data\nHydroShare: Data and models used in hydrology\nOPC: California Ocean Protection Council, marine and coastal datasets\nOSF: The Open Science Foundation, a general location for sharing data and other research tools\n\n\n Exercise and discussion\n\nUsing the tidy dataset, your data dictionary, and your metadata description from the previous exercises, we’ll archive the data on the test node of KNB. This workflow is exactly the same for the actual repository and is designed to get you comfortable adding data to an archive.\n\nLog in to the test archive using your ORCID information: https://dev.nceas.ucsb.edu/\nClick the submit button at the top to start the process of entering your data.\nAdd the relevant metadata information in the forms (because this is a test archive, not all will be entered). The forms will be slightly different from your metadata, but enter the relevant information for the appropriate form.\nSave the dataset when you’re done and marvel at your work.\n\n\n\n\n\nBroman, K. W., and K. H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989.\n\n\nGilliland, A. J. 2016. “Setting the Stage.” In Introduction to Metadata, 3rd ed. Los Angeles, California: Getty Publications.\n\n\nWickham, H. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, H., M. Averick, J. Bryan, W. Chang, L. D’Agostino McGowan, R. François, G. Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, H., and G. Grolemund. 2017. R for Data Science. Sebastopol, California: O’Reilly.\n\n\nZiemann, M., Y. Eren, and A. El-Osta. 2016. “Gene Name Errors Are Widespread in the Scientific Literature.” Genome Biology 17 (1): 1–3. https://doi.org/10.1186/s13059-016-1044-7."
  },
  {
    "objectID": "implement.html#goals-and-motivation",
    "href": "implement.html#goals-and-motivation",
    "title": "4  Lowering barriers to inclusion and addressing key critiques",
    "section": "4.1 Goals and motivation",
    "text": "4.1 Goals and motivation\nWe finish our workshop with a discussion of what it means to use open science in the real world. It’s great to talk about the value of open science and the tools you can use, but it’s a completely different ball game when it comes to putting these ideas into practice. Our goal is that you leave this workshop an advocate and early adopter for the ideas we discussed today - spread these ideas to your peers and colleagues! To realistically achieve this goal, we will talk about some of the challenges you will face so you can develop a realistic expectation of what’s to come.\n\nGoal: Understand common hurdles in adopting open science and how to overcome them\n\nMotivation: Become the “open science” expert at your institution!"
  },
  {
    "objectID": "implement.html#learning-curves",
    "href": "implement.html#learning-curves",
    "title": "4  Lowering barriers to inclusion and addressing key critiques",
    "section": "4.2 Learning curves",
    "text": "4.2 Learning curves\n\n\n\n\n\n\n\n\n\n\n Challenge\n\nIt’s hard to learn new tools!\n\n Solution\n\nIt’s an investment, look to the community!\nYou’ve probably seen a graphic like this if you’ve ever taken a course in R or Python. The hope is that you’re able to quickly reach the land of sunshine and bunnies, but the path is treacherous and even insurmountable for some.\nA huge obstacle in using open science is that the toolsets can have steep learning curves. More popular platforms, such as Excel, are used by many because they’re simple and intuitive. However, as noted earlier, FAIR workflows and tools are sacrificed for ease of use.\nAlthough it’s true that adopting new tools will slow forward progress, this is only temporary. Consider your path towards learning new platforms an investment in your future. The immediate benefit may not be apparent, but you’ll soon wonder how you ever got by before.\nIt’s also helpful to think about the broader community that can support you along this journey. Learning alone can be discouraging and we strongly recommend that you tap into the diverse community of educators, mentors, bloggers, and friends that can help. Even you can create a community of practice!\n\n\n\n\n\n\n\n\n\n\n Exercise and discussion\n\nHow can you engage your peers to develop a shared workspace to learn new tools? What tools will you learn?"
  },
  {
    "objectID": "implement.html#fear-of-exposure",
    "href": "implement.html#fear-of-exposure",
    "title": "4  Lowering barriers to inclusion and addressing key critiques",
    "section": "4.3 Fear of exposure",
    "text": "4.3 Fear of exposure\n\n\n\n\n\n\n\n\n\n\n Challenge\n\nBeing open makes me nervous!\n\n Solution\n\nBeing open helps you collaborate, increases competitiveness, and creates a better scientific product!\nPracticing open science can feel like science in a fish bowl. Although this is kind of the goal, many view this transparency as a liability. Many fear having their ideas “scooped” or losing credibility because of greater exposure of mistakes. These are real concerns that require consideration when working towards more open workflows.\nIn conventional academic settings, competition for resources (e.g., via grant funding) is a real issue and being open can be seen as a risk to the competitive edge. We cannot dismiss this fact, but rather we can think about a lack of openness as a hindrance to forward progress and stifled creativity.\nThink about being open as a means to finding your next collaborator. Creating FAIR data opens the door for others to engage with your science. In fact, being open can increase the competitiveness of research proposals by building a stronger team that collaborates and shares data through better workflows.\nFirst time practitioners of open science also worry about the risk of “airing their dirty laundry”. By exposing the process and potential mistakes, many worry that their integrity as scientists may be questioned.\nThese fears are unfounded as the scientific process by definition is iterative. Hypotheses are supported or refuted through trial and error - if you’re getting your answer after one pass, you’re probably not doing it right. Making the process more transparent can help build trust as your collaborators can better appreciate how decisions and conclusions were made.\nMistakes in research are also very common, much more so than many people realize. By being open, it is true that mistakes are more visible, but this also provides a mechanism for fixing. Being open can lead to a better product by simply having more eyes on the process. It also helps normalize mistakes as part of the process - perfection is an unrealistic expectation.\n\n Exercise and discussion\n\nWhat are your personal concerns about adopting open science?"
  },
  {
    "objectID": "implement.html#what-does-it-mean-to-be-open",
    "href": "implement.html#what-does-it-mean-to-be-open",
    "title": "4  Lowering barriers to inclusion and addressing key critiques",
    "section": "4.4 What does it mean to be open?",
    "text": "4.4 What does it mean to be open?\n\n Challenge\n\nPeople and institutions define open differently!\n\n Solution\n\nUnderstand the context and demonstrate the value!\nAlso realize that open science can mean different things to different people. By extension, this also applies to institutions. We presented the five schools of open science to help conceptualize ideas and tools when we discuss what it means to different groups.\nThink about your employer and what they might care about if you advocate for adoption of open science. Do you need to convince them that there is value in being open? What is their value proposition? What are the hurdles to achieving openness at your institution?\nFor many institutions, being open may come with IT hurdles as you push for alternative software platforms. Working with IT staff to develop trust and comfort for new software may be your burden, but as always, it’s an investment in the future.\nMaybe there are legal contexts to being open. For example, Florida has the “Sunshine” law that makes all government communications public record. What does this mean for using new workflows in open science? Is this is an improvement or a liability (see previous section)?\nIf you’re an administrator or manager, maybe you’re the one that makes the call about being open. It’s important for you to create a culture that promotes and supports open science. Allow space and time for your staff to learn new skills. Realize that investing time in open science is an investment in the future.\n\n Exercise and discussion\n\nWhat does being open mean to you? What do you think being open means to your employer?"
  },
  {
    "objectID": "implement.html#something-is-better-than-nothing",
    "href": "implement.html#something-is-better-than-nothing",
    "title": "4  Lowering barriers to inclusion and addressing key critiques",
    "section": "4.5 Something is better than nothing",
    "text": "4.5 Something is better than nothing\n\n Challenge\n\nDoing all the things is impossible!\n\n Solution\n\nStart small, incremental progress is the name of the game!\nFirst time open science enthusiasts can be overwhelmed by the apparent need to check all the boxes on the open science list. There’s often a prevailing sentiment that you’re not doing open science unless you do all the things. This is simply not true. Just remember that doing something is a huge improvement over doing nothing.\nOpenness in science exists on a spectrum. Your goal should be incremental movement away from the completely closed end of the spectrum. Perhaps you set a goal of only accomplishing one open science task for a particular project. Maybe you start by developing a simple metadata text file or developing a data dictionary. Or maybe you make a commitment to try a new communication platform for collaborative engagement.\nChanneling this concept, Wilson et al. (2017) discuss “good enough practices” in scientific computing, acknowledging that very few of us are professionally trained in these disciplines and sometimes “good enough” is all we can ask for. Lowenberg et al. (2021) also advocate for simple adoption, rather than perfection, when it comes to data citation practices.\nSo, be kind to yourself when learning new skills and realize that the first step will likely be frustration, but through frustration comes experience. The more comfortable you become with a new task, the more likely you’ll be able to attempt additional tasks in the future.\n\n Exercise and discussion\n\nWhat are some simple things you can do to begin adopting open science?\n\n\n\n\nLowenberg, Daniella, Rachael Lammey, Matthew B Jones, John Chodacki, and Martin Fenner. 2021. “Data Citation: Let’s Choose Adoption over Perfection.” Zenodo. https://doi.org/10.5281/zenodo.4701079.\n\n\nWilson, G., J. Bryan, K. Cranston, J. Kitzes, L. Nederbragt, and T. K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLoS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "resources.html#open-science-websites",
    "href": "resources.html#open-science-websites",
    "title": "Resources for continued learning",
    "section": "Open Science Websites",
    "text": "Open Science Websites\n\nNCEAS Open Science for Synthesis workshop\nNCEAS Reproducible Research Techniques\nOpen Science Foundation open science workshop\nOpenscapes\nOpenscapes Champions Lesson Series\nSupercharge your research: A 10 week plan for open data science\nROpenSci guidance on creating a Code of Conduct\nNOAA Reproducible Reporting with R\nPeerJ collection on practical data science"
  },
  {
    "objectID": "resources.html#data-management-tools",
    "href": "resources.html#data-management-tools",
    "title": "Resources for continued learning",
    "section": "Data Management Tools",
    "text": "Data Management Tools\n\nEnvironmental Data Initiative Data Management Resources\nUniversity of California DMPTool\nUS Geological Survey resources for Metadata Creation\nELIXIR and others Data Stewardship Wizard\nTBEP Data Management SOP"
  },
  {
    "objectID": "resources.html#tbep-r-trainings",
    "href": "resources.html#tbep-r-trainings",
    "title": "Resources for continued learning",
    "section": "TBEP R Trainings",
    "text": "TBEP R Trainings\n\nPeconic Estuary Program R training, recording\nTBEP June 2020 R training, recordings\nWriting functions in R\nR package development workflow\nA soft introduction to Shiny"
  },
  {
    "objectID": "resources.html#r-lessons-tutorials",
    "href": "resources.html#r-lessons-tutorials",
    "title": "Resources for continued learning",
    "section": "R Lessons & Tutorials",
    "text": "R Lessons & Tutorials\n\nSoftware Carpentry: R for Reproducible Scientific Analysis\nData Carpentry: Geospatial Workshop\nData Carpentry: R for Data Analysis and Visualization of Ecological Data\nData Carpentry: Data Organization in Spreadsheets\nR for Water Resources Data Science\nRStudio Webinars, many topics\nR For Cats: Basic introduction site, with cats!\nTopical cheatsheets from RStudio, also viewed from the help menu\nCheatsheet from CRAN of base R functions\nTotally awesome R-related artwork by Allison Horst\nColor reference PDF with text names, Color cheatsheet PDF from NCEAS"
  },
  {
    "objectID": "resources.html#r-ebookscourses",
    "href": "resources.html#r-ebookscourses",
    "title": "Resources for continued learning",
    "section": "R eBooks/Courses",
    "text": "R eBooks/Courses\n\nJenny Bryan’s Stat545.com\nGarrett Grolemund and Hadley Wickham’s R For Data Science\nChester Ismay and Albert Y. Kim’s Modern DiveR\nJulia Silge and David Robinson Text Mining with R\nHadley Wickham’s Advanced R\nHadley Wickham’s R for Data Science\nYihui Xie R Markdown: The Definitive Guide\nWinston Chang R Graphics Cookbook\nWegman et al. Remote Sensing and GIS for Ecologists: Using Open Source Software\nLovelace et al. Geocomputation with R\nEdszer Pebesma and Roger Bivand Spatial Data Science"
  },
  {
    "objectID": "resources.html#gitgithub",
    "href": "resources.html#gitgithub",
    "title": "Resources for continued learning",
    "section": "Git/Github",
    "text": "Git/Github\n\nJenny Bryan’s Happy Git and Github for the useR\nGit and GitHub for the Casual User\nCoding Club Intro to Github"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Beck, M. W., C. O’Hara nad J. S. S. Lowndes, R. D. Mazor, S. Theroux, D.\nJ. Gillett, B. Lane, and G. Gearheart. 2020. “The Importance of\nOpen Science for Biological Assessment of Aquatic Environments.”\nPeerJ 8: e9539. https://doi.org/10.7717/peerj.9539.\n\n\nBroman, K. W., and K. H. Woo. 2018. “Data Organization in\nSpreadsheets.” The American Statistician 72 (1): 2–10.\nhttps://doi.org/10.1080/00031305.2017.1375989.\n\n\nFecher, B., and S. Friesike. 2014. “Open Science: One Term, Five\nSchools of Thought.” In Opening Science, 17–47.\nSpringer, Cham.\n\n\nGilliland, A. J. 2016. “Setting the Stage.” In\nIntroduction to Metadata, 3rd ed. Los Angeles, California:\nGetty Publications.\n\n\nHampton, S. E., S. S. Anderson, S. C. Bagby, C. Gries, X. Han, E. M.\nHart, M. B. Jones, et al. 2015. “The Tao of Open Science for\nEcology.” Ecosphere 6 (7): 1–13. https://doi.org/10.1890/ES14-00402.1.\n\n\nLowenberg, Daniella, Rachael Lammey, Matthew B Jones, John Chodacki, and\nMartin Fenner. 2021. “Data Citation: Let’s Choose Adoption over\nPerfection.” Zenodo. https://doi.org/10.5281/zenodo.4701079.\n\n\nLowndes, J. S. S., B. D. Best, C. Scarborough, J. C. Afflerbach, M. R.\nFrazier, C. C. O’Hara, N. Jiang, and B. S. Halpern. 2017. “Our\nPath to Better Science in Less Time Using Open Data Science\nTools.” Nature Ecology & Evolution 1 (0160): 1–7. https://doi.org/10.1038/s41559-017-0160.\n\n\nMichener, W. K., J. W. Brunt, J. J. Helly, T. B. Kirchner, and S. G.\nStafford. 1997. “Nongeospatial Metadata for the Ecological\nSciences.” Ecological Applications 7 (1): 330–42.\nhttps://doi.org/https://doi.org/10.1890/1051-0761(1997)007[0330:NMFTES]2.0.CO;2.\n\n\nWickham, H. 2014. “Tidy Data.” Journal of Statistical\nSoftware 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, H., M. Averick, J. Bryan, W. Chang, L. D’Agostino McGowan, R.\nFrançois, G. Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, H., and G. Grolemund. 2017. R for Data Science.\nSebastopol, California: O’Reilly.\n\n\nWilkinson, M. D., M. Dumontier, I. J. Aalbersberg, G. Appleton, M.\nAxton, A. Baak, N. Blomberg, et al. 2016. “The FAIR Guiding\nPrinciples for Scientific Data Management and Stewardship.”\nScientific Data 3 (160018). https://doi.org/10.1038/sdata.2016.18.\n\n\nWilson, G., J. Bryan, K. Cranston, J. Kitzes, L. Nederbragt, and T. K.\nTeal. 2017. “Good Enough Practices in Scientific\nComputing.” PLoS Computational Biology 13 (6): e1005510.\nhttps://doi.org/10.1371/journal.pcbi.1005510.\n\n\nZiemann, M., Y. Eren, and A. El-Osta. 2016. “Gene Name Errors Are\nWidespread in the Scientific Literature.” Genome Biology\n17 (1): 1–3. https://doi.org/10.1186/s13059-016-1044-7."
  },
  {
    "objectID": "codeofconduct.html#our-pledge",
    "href": "codeofconduct.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Our Pledge",
    "text": "Our Pledge\nWe as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "codeofconduct.html#our-standards",
    "href": "codeofconduct.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Our Standards",
    "text": "Our Standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "codeofconduct.html#enforcement-responsibilities",
    "href": "codeofconduct.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement Responsibilities",
    "text": "Enforcement Responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "codeofconduct.html#scope",
    "href": "codeofconduct.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Scope",
    "text": "Scope\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "codeofconduct.html#enforcement",
    "href": "codeofconduct.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement",
    "text": "Enforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at mbeck@tbep.org. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "codeofconduct.html#enforcement-guidelines",
    "href": "codeofconduct.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement Guidelines",
    "text": "Enforcement Guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "codeofconduct.html#attribution",
    "href": "codeofconduct.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Attribution",
    "text": "Attribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  }
]